{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd163e0f-e8e5-4006-b75f-4028db1fc70b",
   "metadata": {},
   "source": [
    "# Лабораторна робота #3\r",
    "## Знайомство з нейромережами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d0c1df-dbd8-4401-b216-182522ce2b3f",
   "metadata": {},
   "source": [
    "## Завдання роботи\n",
    "1. Повнозв'язані нейронні мережі\n",
    "Вирішіть завдання класифікації даних, з якими ви працювали в лабораторній № 1 за допомогою повнозв’язаної нейромережі прямого поширення (fully connected feed-forward network). Результати порівняйте з одержаними раніше. \n",
    "\n",
    "2. Згорткові нейронні мережі\n",
    "Вирішіть завдання класифікації зображень за допомогою згорткової (convolutional) нейромережі двома способами\n",
    "а) навчить мережу з нуля (from scratch)\n",
    "б) застосуйте перенесення навчання (transfer learning from pre-trained weights)\n",
    "Порівняйте результати (якщо в обраному датасеті класів забагато, достатньо залишити 3-5).\n",
    "\n",
    "3. Рекурентні нейронні мережі\n",
    "Вирішіть задачу класифікації текстів (з якими ви працювали в лабораторній № 2) за допомогою рекурентної нейромережі двома способами:\n",
    "а) навчить мережу і embedding шар з нуля (from scratch)\n",
    "б) використовуючи pretrained word embeddings\n",
    " Результати порівняйте між собою і з одержаними раніш. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f97e7ec-62b4-4ef1-af00-fd723d770aa6",
   "metadata": {},
   "source": [
    "## Хід роботи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e71276-4d9a-40ce-afaa-9cef4730574d",
   "metadata": {},
   "source": [
    "Датасет Лабораторної 1 - https://www.kaggle.com/datasets/erdemtaha/cancer-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b3a36eb6-7237-477f-b1bb-1250ebab1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/Cancer_Data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ac4756ff-a74e-4181-a08f-31efce590d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:\n",
      "['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n",
      "\n",
      "Dataset Size:\n",
      "(569, 33)\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Names:\")\n",
    "print(data.columns.tolist())\n",
    "print(\"\\nDataset Size:\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "64ecddbc-e093-4859-b770-0352ece1c408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surv_status: \n",
      "0 = Benign cancer \n",
      "1 = Malignant cancer\n",
      "\n",
      "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
      "0       1.137721     -2.073379        1.316619   1.051840         1.573525   \n",
      "1       1.889764     -0.352886        1.743936   2.022988        -0.821811   \n",
      "2       1.633253      0.457306        1.621241   1.655445         0.947293   \n",
      "3      -0.777364      0.254758       -0.596575  -0.785572         3.288545   \n",
      "4       1.808147     -1.151437        1.837014   1.936331         0.285480   \n",
      "..           ...           ...             ...        ...              ...   \n",
      "564     2.178338      0.722714        2.128944   2.480174         1.046921   \n",
      "565     1.761509      2.087002        1.672011   1.828758         0.107574   \n",
      "566     0.732551      2.047424        0.703145   0.624834        -0.835332   \n",
      "567     1.898509      2.338441        2.048557   1.840711         1.530827   \n",
      "568    -1.844215      1.223263       -1.851447  -1.398441        -3.106844   \n",
      "\n",
      "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
      "0            3.296358        2.754921             2.579619       2.233557   \n",
      "1           -0.481377       -0.005651             0.567769       0.006728   \n",
      "2            1.061540        1.425133             2.077506       0.949554   \n",
      "3            3.415979        1.994857             1.483863       2.886565   \n",
      "4            0.546981        1.432902             1.460326      -0.004278   \n",
      "..                ...             ...                  ...            ...   \n",
      "564          0.226094        2.027228             2.365175      -0.308770   \n",
      "565         -0.011249        0.733696             1.293217      -0.213387   \n",
      "566         -0.032135        0.066989             0.119267      -0.807697   \n",
      "567          3.284965        3.419167             2.707762       2.152849   \n",
      "568         -1.146316       -1.130856            -1.267296      -0.818703   \n",
      "\n",
      "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
      "0                  2.295121  ...      1.932541      -1.361816   \n",
      "1                 -0.878161  ...      1.850236      -0.371503   \n",
      "2                 -0.400153  ...      1.550565      -0.026196   \n",
      "3                  4.991835  ...     -0.277009       0.131798   \n",
      "4                 -0.567168  ...      1.333198      -1.469317   \n",
      "..                      ...  ...           ...            ...   \n",
      "564               -0.941511  ...      1.947313       0.115510   \n",
      "565               -1.071092  ...      1.575889       2.045643   \n",
      "566               -0.905517  ...      0.581909       1.372947   \n",
      "567                1.064107  ...      2.008514       2.236213   \n",
      "568               -0.565728  ...     -1.428001       0.762145   \n",
      "\n",
      "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
      "0           2.360853    2.092685          1.303618           2.614547   \n",
      "1           1.576664    1.977677         -0.376235          -0.428898   \n",
      "2           1.385176    1.526775          0.524936           1.082658   \n",
      "3          -0.244903   -0.556686          3.385936           3.889744   \n",
      "4           1.376058    1.282156          0.218713          -0.311989   \n",
      "..               ...         ...               ...                ...   \n",
      "564         1.798547    2.107289          0.376199          -0.271961   \n",
      "565         1.461163    1.566936         -0.691207          -0.393317   \n",
      "566         0.600987    0.458848         -0.809322           0.351342   \n",
      "567         2.360853    1.731233          1.426107           3.901181   \n",
      "568        -1.451885   -1.102697         -1.856605          -1.205072   \n",
      "\n",
      "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0           2.133073              2.308490        2.750306   \n",
      "1          -0.139180              1.096303       -0.247928   \n",
      "2           0.869637              1.966513        1.149952   \n",
      "3           2.012285              2.187882        6.049820   \n",
      "4           0.626129              0.737533       -0.873168   \n",
      "..               ...                   ...             ...   \n",
      "564         0.677826              1.639803       -1.365584   \n",
      "565         0.246857              0.742113       -0.536251   \n",
      "566         0.337689              0.421509       -1.109657   \n",
      "567         3.228856              2.302384        1.917733   \n",
      "568        -1.306469             -1.743328       -0.051933   \n",
      "\n",
      "     fractal_dimension_worst  \n",
      "0                   1.946364  \n",
      "1                   0.282848  \n",
      "2                   0.202679  \n",
      "3                   4.958285  \n",
      "4                  -0.398592  \n",
      "..                       ...  \n",
      "564                -0.712032  \n",
      "565                -0.978150  \n",
      "566                -0.319536  \n",
      "567                 2.230298  \n",
      "568                -0.754344  \n",
      "\n",
      "[566 rows x 30 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5508\\3723964999.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[\"diagnosis\"] = data[\"diagnosis\"].replace({\"B\": 0, \"M\": 1})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data[\"diagnosis\"] = data[\"diagnosis\"].replace({\"B\": 0, \"M\": 1})\n",
    "print(\"Surv_status: \\n0 = Benign cancer \\n1 = Malignant cancer\\n\")\n",
    "data = data.drop(columns=['id', \"Unnamed: 32\"])\n",
    "\n",
    "y_column = 'diagnosis'\n",
    "\n",
    "numeric_features = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "data[numeric_features] = data[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Removing outliers using z-score method outliers may deform clusters) 8,10 gives good data look\n",
    "for i in numeric_features:\n",
    "    mean_value = data[i].mean()\n",
    "    std_dev = data[i].std()\n",
    "    lower_bound = mean_value - 10 * std_dev\n",
    "    upper_bound = mean_value + 10 * std_dev\n",
    "    data = data[(data[i] >= lower_bound) & (data[i] <= upper_bound)]\n",
    "\n",
    "# Scaling numeric features\n",
    "scaler = StandardScaler()\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "\n",
    "data[y_column] = data[y_column].astype(int)\n",
    "\n",
    "X = data.drop(y_column, axis=1)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3f73d-4233-4368-af57-5c157b403b6d",
   "metadata": {},
   "source": [
    "## Повнозв'язані нейронні мережі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "45d2652f-cc8b-4cb7-b44f-d39ecf1472b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8bf33ec8-aa8f-425f-a85c-a88e7125f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = data[y_column].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b672b-1067-4070-8cb7-a3837f656d8d",
   "metadata": {},
   "source": [
    "Fully connected neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7b55d8c6-caf9-4f94-ad7c-0a980cd7770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CancerClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)  # First hidden layer with 128 units\n",
    "        self.layer2 = nn.Linear(128, 64)         # Second hidden layer with 64 units\n",
    "        self.output_layer = nn.Linear(64, 1)     # Output layer with 1 unit (binary classification)\n",
    "        self.dropout = nn.Dropout(0.2)           # Dropout layer to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))           # ReLU activation on the first layer\n",
    "        x = self.dropout(x)                      # Dropout after first hidden layer\n",
    "        x = torch.relu(self.layer2(x))           # ReLU activation on the second layer\n",
    "        x = torch.sigmoid(self.output_layer(x))  # sigmoid activation on the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "03efe602-a5eb-4fbb-b10f-3da561215cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CancerClassifier(\n",
      "  (layer1): Linear(in_features=30, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CancerClassifier(X_train.shape[1])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7c214-0770-4da9-9a8a-7e3646571a62",
   "metadata": {},
   "source": [
    "Loss function and optimizer. Binary Cross-Entropy Loss for binary classification and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "190751ab-d2aa-4409-9255-9bae7e68431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132de28-71b6-433a-ae9a-daf255eebdf7",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7eb7ea76-3d23-460d-b653-7979782da66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 0.6064\n",
      "Epoch [10/50], Loss: 0.4956\n",
      "Epoch [15/50], Loss: 0.3829\n",
      "Epoch [20/50], Loss: 0.2766\n",
      "Epoch [25/50], Loss: 0.2012\n",
      "Epoch [30/50], Loss: 0.1461\n",
      "Epoch [35/50], Loss: 0.1191\n",
      "Epoch [40/50], Loss: 0.0957\n",
      "Epoch [45/50], Loss: 0.0818\n",
      "Epoch [50/50], Loss: 0.0760\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor).squeeze()  # squeeze to remove unnecessary dimension\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()  # Backpropagate\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30894d5c-364b-4957-b863-5cdc7fa6c543",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "77c07103-282b-4b1d-8730-b41bd0953dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.37%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode disables dropout\n",
    "\n",
    "with torch.no_grad():  # no gradient calculation for inference\n",
    "    outputs = model(X_test_tensor).squeeze()\n",
    "    predictions = (outputs > 0.5).float()  # Convert to binary\n",
    "\n",
    "accuracy = (predictions == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b289c890-0fde-4ffc-be42-7ecea9634df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a859d-3017-4b68-baa2-3e031730eac6",
   "metadata": {},
   "source": [
    "Lets take a look at predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "013f6920-141e-4847-9db2-65bb359b53db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 1.0, True label: 1\n",
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 1.0, True label: 1\n",
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 0.0, True label: 0\n",
      "Prediction: 0.0, True label: 0\n"
     ]
    }
   ],
   "source": [
    "new_predictions = model(X_test_tensor[:10]).squeeze()\n",
    "new_predictions = (new_predictions > 0.5).float()\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Prediction: {new_predictions[i].item()}, True label: {y_test_tensor[i].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c180dea4-d870-4ba1-afa7-442486e20b84",
   "metadata": {},
   "source": [
    "## Згорткові нейронні мережі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cac9d422-53bb-4af0-b841-a605a80b5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b99d0c67-c158-4bcd-8b55-f46f1425d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7590493c-7450-48ab-a838-e3280ea74f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLightDataset(Dataset):\n",
    "    def __init__(self, images_dir, annotations_file, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.num_samples = NUM_SAMPLES\n",
    "        \n",
    "        with open(annotations_file, 'r') as f:\n",
    "            self.annotations = json.load(f)[\"annotations\"]\n",
    "        \n",
    "        self.image_files = [\n",
    "            ann[\"filename\"].replace(\"\\\\\", \"/\").replace(\"train_images/train_images\", \"train_images\")\n",
    "            for ann in self.annotations\n",
    "        ]\n",
    "        \n",
    "        self.image_files = self.image_files[:self.num_samples]\n",
    "        self.annotations = self.annotations[:self.num_samples]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Warning: Image {image_path} not found!\")\n",
    "            return None, None\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        ann = [a for a in self.annotations if a[\"filename\"] == self.image_files[idx]]\n",
    "        \n",
    "        labels = torch.zeros(3)\n",
    "        \n",
    "        for obj in ann:\n",
    "            for light in obj[\"inbox\"]:\n",
    "                if light[\"ignore\"] == 0:  # Only visible lights\n",
    "                    color = light[\"color\"]\n",
    "                    if color == \"red\":\n",
    "                        labels[0] = 1\n",
    "                    elif color == \"green\":\n",
    "                        labels[1] = 1\n",
    "                    elif color == \"yellow\":\n",
    "                        labels[2] = 1\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c68be4ff-e93f-441e-86e9-493bcde6be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = './data/images/train_dataset/'\n",
    "annotations_file = './data/images/train_dataset/train.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5da768-58a3-4fa6-bcc4-02abdf9bdad0",
   "metadata": {},
   "source": [
    "Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "48502360-fed2-478e-b210-af3dc85f6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "40fa5f15-e39c-485e-bccf-7c5897cf04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficLightDataset(\n",
    "    images_dir=images_dir,\n",
    "    annotations_file=annotations_file,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7837f45a-7278-41e7-acae-c60f3c66ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "200c8cf4-7519-4f1d-9253-74ea8e3e5d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 500\n",
      "Label: tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in dataset: {len(train_dataset)}\")\n",
    "image, label = train_dataset[0]\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8cb98-2a80-4f98-b7e3-700525248dd2",
   "metadata": {},
   "source": [
    "#### a) Training From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6574a966-5132-428b-b2b4-0b89e8675674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLightCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(TrafficLightCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(torch.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 128 * 28 * 28)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f7bbb-b700-46c4-8744-6ffe47c00337",
   "metadata": {},
   "source": [
    "Model, loss function and optimizer initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5649e9-e597-4ccc-8b2c-2eeca4533322",
   "metadata": {},
   "source": [
    "3 output classes: red, green, yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "efed3d0c-d74d-4089-9bec-a935508a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrafficLightCNN(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "30a06fe7-2ce4-447c-ba8d-71530f6cdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec8eb7-9cdf-46f7-89d0-f123bced9a72",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy Loss with logits and Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a970bac-b753-4694-a6bc-2de5b7d44171",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9ea78533-8f8a-46f2-8502-306c348437df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.0438\n",
      "Epoch [2/3], Loss: 0.0000\n",
      "Epoch [3/3], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if images is None or labels is None:\n",
    "            continue\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777f1f6-c5e1-4eab-9b8d-ac39b6ec80c0",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5fc00686-195b-4db9-a93b-f65647f8d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # sigmoid is applied to convert logits to probabilities (gives probabilities for each label)\n",
    "        probas = torch.sigmoid(outputs)\n",
    "\n",
    "        # threshold to get binary predictions\n",
    "        predicted = (probas > 0.5).float()\n",
    "        \n",
    "        correct_labels = (predicted == labels).sum(dim=1)\n",
    "        correct += correct_labels.sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Final accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd7e7f-fbf7-41e6-ab86-692227215f26",
   "metadata": {},
   "source": [
    "#### b) Transfer learning from pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "12629936-5306-4bfb-ba5e-47a5b6aad512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# final fully connected layer should match the number of output classes\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# freeze the convolutional layers (only train the fully connected layer initially)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze the fully connected layer\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49764ba1-392d-4c0f-812e-ef0ac25c2ec4",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy Loss with logits and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2b64363a-7c24-4389-aea7-d1dcc7fb08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3be0d-3874-4dd5-b982-15005b28048f",
   "metadata": {},
   "source": [
    "Resize to shape required by ResNet, convert image to tensor and normalize as ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1646b13f-3dcb-47ee-8058-1c344f0fcee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0a1296f8-8290-40e3-aa31-dd44210dda24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrafficLightDataset(\n",
    "    images_dir='./data/images/train_dataset/',\n",
    "    annotations_file='./data/images/train_dataset/train.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921971d-81d8-4c90-b809-b91dfc96ca02",
   "metadata": {},
   "source": [
    "Training loop for transfer learning (initially training the fully connected layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "60ad3722-be84-4894-85cc-5de01fcd945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0015\n",
      "Epoch [2/5], Loss: 0.0013\n",
      "Epoch [3/5], Loss: 0.0011\n",
      "Epoch [4/5], Loss: 0.0010\n",
      "Epoch [5/5], Loss: 0.0009\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if images is None or labels is None:\n",
    "            continue\n",
    "\n",
    "        # move tensors to the device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab64dff-a11d-4935-92db-53a1c5458b28",
   "metadata": {},
   "source": [
    "Unfreeze the model to fine-tune all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9896765a-901f-487c-b6cd-ff064ebcf44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e86de-16d0-4bec-8317-a1ef1354c80b",
   "metadata": {},
   "source": [
    "Re-initialize the optimizer to update all layers and set smaller learning rate for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3eaf0990-2433-4a41-b9e0-8a1f28c24d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5b4b5f88-dd6b-4388-8a89-34dbb352c704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Epoch [1/5], Loss: 0.0005\n",
      "Fine-tuning Epoch [2/5], Loss: 0.0001\n",
      "Fine-tuning Epoch [3/5], Loss: 0.0001\n",
      "Fine-tuning Epoch [4/5], Loss: 0.0000\n",
      "Fine-tuning Epoch [5/5], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if images is None or labels is None:\n",
    "            continue\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Fine-tuning Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "978a9448-fde9-4f9c-8b80-9fb89c8de352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        probas = torch.sigmoid(outputs)\n",
    "\n",
    "        predicted = (probas > 0.5).float()\n",
    "\n",
    "        correct_labels = (predicted == labels).sum(dim=1)\n",
    "        correct += correct_labels.sum().item()\n",
    "        total += labels.numel()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Final accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ada618-7ac8-4deb-a24b-f0ff69e72f2e",
   "metadata": {},
   "source": [
    "### 3. Рекурентні нейронні мережі"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad192c50-5af0-4862-890f-b1f247724437",
   "metadata": {},
   "source": [
    "Вирішіть задачу класифікації текстів (з якими ви працювали в лабораторній № 2) за допомогою рекурентної нейромережі двома способами:\n",
    "\n",
    "а) навчить мережу і embedding шар з нуля (from scratch)\n",
    "\n",
    "б) використовуючи pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e4ea8d5a-0dc6-4099-8d04-10e169ecbf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "325dd735-5177-4183-91d1-71df3a1ba46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "19f4e4b8-56bb-4b94-b7d9-1bf57e279476",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('data/yelp_academic_dataset_review.json', lines=True, nrows=90000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cce6066a-8776-4596-9e3d-56bf8be2c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text, custom_stopwords=None):\n",
    "    if custom_stopwords is None:\n",
    "        custom_stopwords = set()\n",
    "    else:\n",
    "        custom_stopwords = set(custom_stopwords)\n",
    "    \n",
    "    all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in all_stopwords]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b446111b-3f9e-4455-b158-1b0e376da996",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['place', 'one', 'got', 'food', 'drink', 'good', 'restaurant', 'said', 'even', 'told', 'order', 'ordered',\n",
    "                    'really', 'table', 'thing', 'though', 'think', 'menu', 'came', 'go', 'went', 'location', 'pizza', 'time',\n",
    "                    'ask', 'asked', 'people', 'always', 'know', 'still', 'u', 'two', 'three', 'us', 'way', 'service', 'make',\n",
    "                    'going', 'first', 'second', 'third']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f3caf455-ff78-4643-a5b1-ac87ebef881d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If you decide to eat here, just be aware it is...</td>\n",
       "      <td>decide eat aware take hours beginning end trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've taken a lot of spin classes over the year...</td>\n",
       "      <td>taken lot spin classes years nothing compares ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>family diner buffet eclectic assortment large ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>wow yummy different delicious favorite lamb cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>cute interior owner gave tour upcoming area gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If you decide to eat here, just be aware it is...   \n",
       "1  I've taken a lot of spin classes over the year...   \n",
       "2  Family diner. Had the buffet. Eclectic assortm...   \n",
       "3  Wow!  Yummy, different,  delicious.   Our favo...   \n",
       "4  Cute interior and owner (?) gave us tour of up...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  decide eat aware take hours beginning end trie...  \n",
       "1  taken lot spin classes years nothing compares ...  \n",
       "2  family diner buffet eclectic assortment large ...  \n",
       "3  wow yummy different delicious favorite lamb cu...  \n",
       "4  cute interior owner gave tour upcoming area gr...  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(lambda x: preprocess(x, custom_stopwords))\n",
    "\n",
    "data[['text', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce7b2e-b0b4-4397-9112-079fad24913a",
   "metadata": {},
   "source": [
    "#### а) навчить мережу і embedding шар з нуля (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "27dfdaba-51f8-44c7-bb70-8a327b2d3c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "32ae1023-9be7-47ed-bf39-af38ef4a5e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decide eat aware take hours beginning end trie...</td>\n",
       "      <td>[decide, eat, aware, take, hours, beginning, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>taken lot spin classes years nothing compares ...</td>\n",
       "      <td>[taken, lot, spin, classes, years, nothing, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>family diner buffet eclectic assortment large ...</td>\n",
       "      <td>[family, diner, buffet, eclectic, assortment, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow yummy different delicious favorite lamb cu...</td>\n",
       "      <td>[wow, yummy, different, delicious, favorite, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cute interior owner gave tour upcoming area gr...</td>\n",
       "      <td>[cute, interior, owner, gave, tour, upcoming, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  decide eat aware take hours beginning end trie...   \n",
       "1  taken lot spin classes years nothing compares ...   \n",
       "2  family diner buffet eclectic assortment large ...   \n",
       "3  wow yummy different delicious favorite lamb cu...   \n",
       "4  cute interior owner gave tour upcoming area gr...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [decide, eat, aware, take, hours, beginning, e...  \n",
       "1  [taken, lot, spin, classes, years, nothing, co...  \n",
       "2  [family, diner, buffet, eclectic, assortment, ...  \n",
       "3  [wow, yummy, different, delicious, favorite, l...  \n",
       "4  [cute, interior, owner, gave, tour, upcoming, ...  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "data['tokens'] = data['cleaned_text'].apply(tokenize)\n",
    "\n",
    "data[['cleaned_text', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "279e44d8-2d26-489e-b360-424147e6affe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61480"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter([token for tokens in data['tokens'] for token in tokens])\n",
    "vocab = {word: idx+1 for idx, (word, _) in enumerate(counter.most_common())}  # reserve index 0 for padding\n",
    "vocab['<unk>'] = len(vocab) + 1\n",
    "\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8f443f86-0065-4705-9fa8-a3670e358490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[decide, eat, aware, take, hours, beginning, e...</td>\n",
       "      <td>[1290, 39, 1962, 46, 244, 2019, 272, 79, 827, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[taken, lot, spin, classes, years, nothing, co...</td>\n",
       "      <td>[596, 69, 3275, 1308, 128, 117, 5059, 1308, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[family, diner, buffet, eclectic, assortment, ...</td>\n",
       "      <td>[145, 903, 502, 2169, 2460, 135, 16, 2503, 97,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[wow, yummy, different, delicious, favorite, l...</td>\n",
       "      <td>[518, 394, 103, 14, 78, 684, 665, 5303, 103, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[cute, interior, owner, gave, tour, upcoming, ...</td>\n",
       "      <td>[459, 945, 241, 187, 409, 6033, 38, 1, 274, 29...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [decide, eat, aware, take, hours, beginning, e...   \n",
       "1  [taken, lot, spin, classes, years, nothing, co...   \n",
       "2  [family, diner, buffet, eclectic, assortment, ...   \n",
       "3  [wow, yummy, different, delicious, favorite, l...   \n",
       "4  [cute, interior, owner, gave, tour, upcoming, ...   \n",
       "\n",
       "                                             indices  \n",
       "0  [1290, 39, 1962, 46, 244, 2019, 272, 79, 827, ...  \n",
       "1  [596, 69, 3275, 1308, 128, 117, 5059, 1308, 13...  \n",
       "2  [145, 903, 502, 2169, 2460, 135, 16, 2503, 97,...  \n",
       "3  [518, 394, 103, 14, 78, 684, 665, 5303, 103, 1...  \n",
       "4  [459, 945, 241, 187, 409, 6033, 38, 1, 274, 29...  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_indices(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "data['indices'] = data['tokens'].apply(lambda x: text_to_indices(x, vocab))\n",
    "\n",
    "data[['tokens', 'indices']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c4994763-c054-475d-bc44-9b2475d3ffa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(sequences, max_len):\n",
    "    return [seq + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n",
    "\n",
    "max_sequence_length = 100\n",
    "\n",
    "X = pad_sequences(data['indices'], max_sequence_length)\n",
    "\n",
    "len(X[0])  # = max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce166097-c6a5-4d5b-8118-36d40b0c4598",
   "metadata": {},
   "source": [
    "Convert the input data (X) and labels (y) to PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "53994c36-1919-4055-bb4b-8adb32703586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([90000, 100]), torch.Size([90000]))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(data['stars'].values - 1)\n",
    "\n",
    "X_tensor.shape, y_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4458f441-9565-46bc-b03f-6d078c57174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for inputs, labels in train_loader:\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d6372793-9bf9-424f-99f3-912010de2096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        # fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # for simpicity use the last time step (last RNN output)\n",
    "        rnn_out = rnn_out[:, -1, :]\n",
    "        \n",
    "        out = self.fc(rnn_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "53fc473b-6abe-470a-aa24-103740d45095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (embedding): Embedding(61481, 100)\n",
      "  (rnn): RNN(100, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = len(data['stars'].unique())\n",
    "\n",
    "model = RNNModel(len(vocab)+1, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f5215d83-14b4-4348-8c29-0f88bb004a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.4043, Accuracy: 44.09%\n",
      "Epoch 2/5, Loss: 1.3925, Accuracy: 44.53%\n",
      "Epoch 3/5, Loss: 1.3865, Accuracy: 45.23%\n",
      "Epoch 4/5, Loss: 1.3750, Accuracy: 45.87%\n",
      "Epoch 5/5, Loss: 1.3606, Accuracy: 46.60%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "65d48600-320d-4a3d-a7b6-d5145baadbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 47.49%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(model, train_loader)\n",
    "print(f'Training Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "11ae184c-088e-45da-a42c-866b0152f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '2a_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f576b08-da71-45b3-a600-3f1fdc2335e1",
   "metadata": {},
   "source": [
    "#### б) pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "df0ae114-f6bc-40a4-abad-466e6b896921",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('data/yelp_academic_dataset_review.json', lines=True, nrows=90000)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text, custom_stopwords=None):\n",
    "    if custom_stopwords is None:\n",
    "        custom_stopwords = set()\n",
    "    else:\n",
    "        custom_stopwords = set(custom_stopwords)\n",
    "    \n",
    "    all_stopwords = stop_words.union(custom_stopwords)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in all_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "custom_stopwords = ['place', 'one', 'got', 'food', 'drink', 'good', 'restaurant', 'said', 'even', 'told', 'order', 'ordered',\n",
    "                    'really', 'table', 'thing', 'though', 'think', 'menu', 'came', 'go', 'went', 'location', 'pizza', 'time',\n",
    "                    'ask', 'asked', 'people', 'always', 'know', 'still', 'u', 'two', 'three', 'us', 'way', 'service', 'make',\n",
    "                    'going', 'first', 'second', 'third']\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(lambda x: preprocess(x, custom_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "573d54f0-6afc-4c08-ac1e-88a3dad9308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file_path = './glove.6B.100d.txt'\n",
    "embeddings_index = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "print(f'Loaded {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dd1301ca-ca74-4bc6-8377-1a18b0302490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['cleaned_text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(data['cleaned_text'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if word in embeddings_index:\n",
    "        embedding_matrix[index] = embeddings_index[word]\n",
    "\n",
    "max_sequence_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4ae3744e-185a-48d4-a1f4-9314099c947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = data['stars'].values\n",
    "y = to_categorical(y - 1, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "43228362-5b25-41e2-b63d-d15fac4869b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 72000 samples\n",
      "Validation set size: 18000 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Validation set size: {X_val.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5960e9f3-e0ad-4b1a-ac97-f9100cd7b980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">6,148,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m6,148,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,148,000</span> (23.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,148,000\u001b[0m (23.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,148,000</span> (23.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m6,148,000\u001b[0m (23.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e9801f84-5380-46ef-92da-a75cf5b6a4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 39ms/step - accuracy: 0.5230 - loss: 1.1303 - val_accuracy: 0.5908 - val_loss: 0.9754\n",
      "Epoch 2/5\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 39ms/step - accuracy: 0.6002 - loss: 0.9395 - val_accuracy: 0.5880 - val_loss: 0.9433\n",
      "Epoch 3/5\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 39ms/step - accuracy: 0.6181 - loss: 0.8907 - val_accuracy: 0.6237 - val_loss: 0.8757\n",
      "Epoch 4/5\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 39ms/step - accuracy: 0.6336 - loss: 0.8543 - val_accuracy: 0.6287 - val_loss: 0.8606\n",
      "Epoch 5/5\n",
      "\u001b[1m2250/2250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 41ms/step - accuracy: 0.6451 - loss: 0.8267 - val_accuracy: 0.6228 - val_loss: 0.8730\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "997c24dc-d68d-47c2-ad52-2ab3ffd6b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.6232 - loss: 0.8711\n",
      "Validation Loss: 0.8730288147926331\n",
      "Validation Accuracy: 0.6228333115577698\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "24adda86-fd51-4056-af56-47393005648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted ratings: [5 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "manual_reviews = [\"This place is amazing!\", \"The food was awful, I would not recommend.\", \"Food here is as good as one you would find in a garbage\", \"The place was amazingly bad\"]\n",
    "manual_reviews_cleaned = [preprocess(review) for review in manual_reviews]\n",
    "manual_reviews_sequences = tokenizer.texts_to_sequences(manual_reviews_cleaned)\n",
    "manual_reviews_padded = pad_sequences(manual_reviews_sequences, maxlen=100)\n",
    "\n",
    "predictions = model.predict(manual_reviews_padded)\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis=1) + 1\n",
    "print(f\"Predicted ratings: {predicted_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde40ba5-bbe9-4c70-843d-f2d0a6bad8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
